{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import demoji\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pattern.text.en import singularize\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, curse_words_path, contractions_path, user_info_path) -> None:\n",
    "        self.__contractions = self.__load_contractions(contractions_path)\n",
    "        self.__curse_words = self.__load_curse_words(curse_words_path)\n",
    "        self.__stop_words = self.__load_stopwords()\n",
    "        self.__word_dictionary = self.__load_word_dict()\n",
    "        self.__user_info = self.__load_user_info(user_info_path)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def __load_contractions(self, path):\n",
    "       return json.loads(open(path, \"r\").read())\n",
    "\n",
    "    def __load_curse_words(self, path):\n",
    "      curse_words = open(path, 'r')\n",
    "      return curse_words.read()\n",
    "\n",
    "    def __load_stopwords(self):\n",
    "       return nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    def __load_word_dict(self):\n",
    "      return set(nltk.corpus.words.words())\n",
    "\n",
    "    def __load_user_info(self, path):\n",
    "      return pd.read_csv(path)\n",
    "\n",
    "    def __get_wordnet_pos(self, word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV\n",
    "        }\n",
    "        # Noun is the default if the tag is not found\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    def __plot_word_frequency(self, texts):\n",
    "        combined_text = ' '.join(texts)\n",
    "        words = combined_text.split()\n",
    "        word_freq = Counter(words)\n",
    "        most_common_words = word_freq.most_common(20)\n",
    "        words = [word for word, _ in most_common_words]\n",
    "        frequencies = [freq for _, freq in most_common_words]\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(words)))\n",
    "\n",
    "        # Plot the histogram\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(words, frequencies, color=colors)\n",
    "        plt.xlabel('Words')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Word Frequency Histogram (Top 20)')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def __clean_tweet(self, tweet, label):\n",
    "        text = re.sub(r\"[^\\w\\s']|_\", \"\", tweet['text'])\n",
    "        text = re.sub(r\"(?<!\\w)'|'(?!\\w)\", \"\", text)\n",
    "\n",
    "        # Remove links\n",
    "        text = re.sub(r'http\\S+|www\\S+|\\S+\\.com\\S+', '', text)\n",
    "        # Remove user mentions with @\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'^[Rr][Tt]\\s+', '', text)\n",
    "\n",
    "        for word in text.split():\n",
    "          if word.lower() in self.__contractions:\n",
    "            text = text.replace(word, self.__contractions[word.lower()])\n",
    "\n",
    "        # tokenizing tweet\n",
    "        tokenized_text = word_tokenize(text)\n",
    "        i = 0\n",
    "        while i < len(tokenized_text):\n",
    "          tokenized_text[i] = tokenized_text[i].lower()\n",
    "          # changing verbs to its base form\n",
    "          tokenized_text[i] = WordNetLemmatizer().lemmatize(tokenized_text[i], self.__get_wordnet_pos(tokenized_text[i]))\n",
    "          # changing plural nouns to their singular forms\n",
    "          tokenized_text[i] = singularize(tokenized_text[i])\n",
    "\n",
    "          # removing all single chars from tweets except \"I\"\n",
    "          if len(tokenized_text[i]) == 1 and \\\n",
    "                  tokenized_text[i] != 'i':\n",
    "            del tokenized_text[i]\n",
    "          elif tokenized_text[i].lower() == 'thi':\n",
    "            del tokenized_text[i]\n",
    "          # removing stopwords\n",
    "          elif tokenized_text[i] in self.__stop_words:\n",
    "            del tokenized_text[i]\n",
    "          # removing all meaningless texts\n",
    "          elif tokenized_text[i].lower() not in self.__word_dictionary and\\\n",
    "                  tokenized_text[i].lower() not in self.__curse_words:\n",
    "            del tokenized_text[i]\n",
    "          else:\n",
    "            i += 1\n",
    "        return [label, ' '.join(tokenized_text)]\n",
    "\n",
    "    def __check_folder_exists(self, folder_name):\n",
    "      if not os.path.isdir(folder_name):\n",
    "          os.makedirs(folder_name)\n",
    "\n",
    "    def __save_images(self, url, file_name, label, data_set):\n",
    "      try:\n",
    "          if label == 0:\n",
    "            folder_name = str(label) + data_set\n",
    "          elif label == 1:\n",
    "            folder_name = str(label) + data_set\n",
    "          elif label == None:\n",
    "            folder_name = str(label) + data_set\n",
    "          else:\n",
    "            print('invalide label!')\n",
    "            return\n",
    "\n",
    "          self.__check_folder_exists(folder_name)\n",
    "          file_name = file_name + '.jpg'\n",
    "          full_filename = os.path.join(folder_name, file_name)\n",
    "          # downloading image from the link\n",
    "          urllib.request.urlretrieve(url, full_filename)\n",
    "          return True\n",
    "      except BaseException as err:\n",
    "          return False\n",
    "\n",
    "    def download_shen_images(self, path, label):\n",
    "        data_set = \"\"\n",
    "        if(label == 1):\n",
    "          data_set = \"shen_postive\"\n",
    "        else:\n",
    "          data_set = \"shen_negative\"\n",
    "        files = glob.glob(path, recursive=True)\n",
    "        for file in files:\n",
    "            with open(file) as f:\n",
    "                json_file = json.load(f)\n",
    "                if 'media' in json_file['entities']:\n",
    "                    for media in json_file['entities']['media']:\n",
    "                        self.__save_images(media['media_url_https'],\n",
    "                                          media['id_str'], label, data_set=data_set)\n",
    "\n",
    "    def download_clpsych_images(self, path):\n",
    "      example_usernames = [x.split('.')[0] for x in os.listdir(path)]\n",
    "\n",
    "      def tweet_processor(x):\n",
    "          return json.loads(x)\n",
    "      counter = 0\n",
    "      for username in example_usernames:\n",
    "          IN = open(path + username + '.tweets')\n",
    "          tweets = filter(None, map(tweet_processor, IN))\n",
    "          for tweet in tweets:\n",
    "              label = label = self.__user_info.loc[self.__user_info['anonymized_screen_name']\n",
    "                                                  == tweet['user']['screen_name']]\n",
    "              # print(label)\n",
    "              screen_name = label.iloc[0][0]\n",
    "              label = label.iloc[0][4]\n",
    "              data_set = \"\"\n",
    "              if(label == 1):\n",
    "                data_set = \"clp_postive\"\n",
    "              else:\n",
    "                data_set = \"clp_negative\"\n",
    "                \n",
    "              if 'media' in tweet['entities'].keys():\n",
    "                  self.__save_images(tweet['entities']['media'][0]\n",
    "                                    ['media_url_https'], screen_name + str(counter), label, data_set=data_set)\n",
    "                  counter = counter + 1\n",
    "\n",
    "    def extract_emojis(self, tweet):\n",
    "        if tweet['lang'] == 'en':\n",
    "          all_emojis = ''.join(\n",
    "              c for c in tweet['text'] if c in emoji.distinct_emoji_list(tweet['text']))\n",
    "          emojis_meaning = demoji.findall(all_emojis)\n",
    "          return emojis_meaning\n",
    "\n",
    "    def __combine_tweets(self, p_tweets, n_tweets):\n",
    "        all_tweets = pd.concat([p_tweets, n_tweets])\n",
    "        all_tweets = all_tweets.sample(frac=1)\n",
    "        return all_tweets\n",
    "\n",
    "    # visualizing tweets\n",
    "\n",
    "    def __visualise_tweets(self, tweet_list):\n",
    "        tweet_list = ' '.join(tweet_list)\n",
    "        Mask = np.array(Image.open('./Twitter-PNG-Image.png'))\n",
    "        image_colors = ImageColorGenerator(Mask)\n",
    "        wc = WordCloud(background_color='black', height=1500,\n",
    "                      width=4000, mask=Mask).generate(tweet_list)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(wc.recolor(color_func=image_colors),\n",
    "                  interpolation=\"hamming\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def __load(self, path, label, dataset):\n",
    "      non_cleaned_tweets = []\n",
    "      tweets_list = []\n",
    "      tweets_emojis = []\n",
    "      if dataset == 'shen':\n",
    "        # loading file from given path\n",
    "        files = glob.glob(path, recursive=True)\n",
    "        # print(len(files))\n",
    "        for file in files:\n",
    "          with open(file) as f:\n",
    "            tweet = json.load(f)\n",
    "            if tweet['lang'] == 'en':\n",
    "              non_cleaned_tweets.append(tweet['text'])\n",
    "              tweet_text = self.__clean_tweet(tweet, label)\n",
    "              if len(tweet_text) > 1:\n",
    "                tweets_list.append(tweet_text)\n",
    "              tweet_emojis = self.extract_emojis(tweet)\n",
    "              if len(tweet_emojis) > 0:\n",
    "                temp = [label, ' '.join(list(tweet_emojis.values()))]\n",
    "                tweets_emojis.append(temp)\n",
    "\n",
    "      if dataset == 'clpsych':\n",
    "        example_usernames = [x.split('.')[0] for x in os.listdir(path)]\n",
    "\n",
    "        def tweet_processor(x):\n",
    "            return json.loads(x)\n",
    "        j = 0\n",
    "        print('Loading tweets...')\n",
    "        for username in example_usernames:\n",
    "            j = j + 1\n",
    "            # print(username + ' and umber is ',j)\n",
    "            IN = open(path + username + '.tweets')\n",
    "            tweets = filter(None, map(tweet_processor, IN))\n",
    "            tweet_counter = 0\n",
    "            for tweet in tweets:\n",
    "                if tweet_counter == 100:\n",
    "                  break\n",
    "                tweet_counter += 1\n",
    "                if tweet['lang'] == 'en':\n",
    "                  label = self.__user_info.loc[self.__user_info['anonymized_screen_name']\n",
    "                                              == tweet['user']['screen_name']]\n",
    "                  label = label.iloc[0][4]\n",
    "                  if label == 'ptsd' or 'condition':\n",
    "                    continue\n",
    "                  # print(label)\n",
    "                  if label == 'control':\n",
    "                    label = 0\n",
    "                  if label == 'depression':\n",
    "                    label = 1\n",
    "                  print(tweet)\n",
    "                  tweet_text = self.__clean_tweet(tweet, label)\n",
    "                  if len(tweet_text[1].split(' ')) > 1:\n",
    "                    tweets_list.append(tweet_text)\n",
    "                    # print(tweet_text)\n",
    "                  tweet_emojis = self.extract_emojis(tweet)\n",
    "                  if len(tweet_emojis) > 0:\n",
    "                    temp = [label, ' '.join(list(tweet_emojis.values()))]\n",
    "                    # print(temp)\n",
    "                    tweets_emojis.append(temp)\n",
    "\n",
    "      print('Before pre-processing')              \n",
    "      self.__visualise_tweets(non_cleaned_tweets)\n",
    "      print('Before pre-processing')\n",
    "      self.__plot_word_frequency(non_cleaned_tweets)\n",
    "      tweets_list = pd.DataFrame(tweets_list, columns=['Label', 'Tweet'])\n",
    "      print('After pre-processing')\n",
    "      self.__visualise_tweets(list(tweets_list['Tweet']))\n",
    "      print('After pre-processing')\n",
    "      self.__plot_word_frequency(list(tweets_list['Tweet']))\n",
    "      tweets_emojis = pd.DataFrame(tweets_emojis, columns=[\n",
    "                                  'Label', 'EmojisMeaning'])\n",
    "      print(tweets_list)\n",
    "      return tweets_list, tweets_emojis\n",
    "\n",
    "    def load_tweets_shen(self, p_labeled_path, n_labeled_path):\n",
    "      print('Depressed users tweets')\n",
    "      p_tweets, p_emojis = self.__load(p_labeled_path, 0, 'shen')\n",
    "      print('Non-depressed users tweets')\n",
    "      n_tweets, n_emojis = self.__load(n_labeled_path, 1, 'shen')\n",
    "      all_tweets = self.__combine_tweets(p_tweets, n_tweets)\n",
    "      all_emojis = self.__combine_tweets(p_emojis, n_emojis)\n",
    "      all_tweets.to_csv('shen_all_tweets.csv')\n",
    "      all_emojis.to_csv('shen_all_emojis.csv')\n",
    "      return all_tweets, all_emojis\n",
    "\n",
    "    def load_tweets_clpsych(self, path):\n",
    "        all_tweets, all_emojis = self.__load(path, None, 'clpsych')\n",
    "        all_tweets.to_csv('clpsych_all_tweets')\n",
    "        all_emojis.to_csv('clpsych_all_emojis')\n",
    "        return all_tweets, all_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIT_Caption:\n",
    "    def __init__(self) -> None:\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            \"microsoft/git-base-coco\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"microsoft/git-base-coco\")\n",
    "\n",
    "    def __extract_caption(self, imgs_path):\n",
    "        generated_captions = []\n",
    "        for img in imgs_path:\n",
    "            image = Image.open(img)\n",
    "\n",
    "            pixel_values = self.processor(\n",
    "                images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "            generated_ids = self.model.generate(\n",
    "                pixel_values=pixel_values, max_length=50)\n",
    "            generated_captions.append(self.processor.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "        return generated_captions\n",
    "\n",
    "    def generate_captions(self, imgs_path, label):\n",
    "        imgs = glob.glob(imgs_path, recursive=True)\n",
    "        captions = self.__extract_caption(imgs)\n",
    "        labels = np.full(len(imgs), label)\n",
    "        data = {'Label': labels, 'Captions': captions}\n",
    "        captions_df = pd.DataFrame(data)\n",
    "        return captions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = Preprocessing('./curse_words.txt', './contractions.json',\n",
    "                   './anonymized_user_info_by_chunk.csv')\n",
    "pr.load_tweets_clpsych('./0/')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_generator = GIT_Caption()\n",
    "cap_generator.generate_captions(\"./shen_positive\")\n",
    "cap_generator.generate_captions(\"./shen_negative\")\n",
    "cap_generator.generate_captions(\"./clp_positive\")\n",
    "cap_generator.generate_captions(\"./clp_negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
